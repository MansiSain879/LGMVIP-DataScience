{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Next Word Prediction**"
      ],
      "metadata": {
        "id": "FSK0S6m9qVnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Mansi Sain**"
      ],
      "metadata": {
        "id": "0ptZsEbGqQ4c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lajJ9P7YBXjG",
        "outputId": "153bc540-d174-4c39-c467-9303317d3878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 42ms/step - accuracy: 0.0675 - loss: 6.5455\n",
            "Epoch 2/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 43ms/step - accuracy: 0.1286 - loss: 5.4385\n",
            "Epoch 3/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 41ms/step - accuracy: 0.1632 - loss: 4.9353\n",
            "Epoch 4/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 42ms/step - accuracy: 0.1855 - loss: 4.5478\n",
            "Epoch 5/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 44ms/step - accuracy: 0.2147 - loss: 4.1765\n",
            "Epoch 6/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m139s\u001b[0m 44ms/step - accuracy: 0.2491 - loss: 3.8328\n",
            "Epoch 7/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m138s\u001b[0m 43ms/step - accuracy: 0.2893 - loss: 3.4991\n",
            "Epoch 8/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 42ms/step - accuracy: 0.3356 - loss: 3.1906\n",
            "Epoch 9/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m135s\u001b[0m 43ms/step - accuracy: 0.3813 - loss: 2.9149\n",
            "Epoch 10/10\n",
            "\u001b[1m3176/3176\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 43ms/step - accuracy: 0.4272 - loss: 2.6625\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the text data from a file\n",
        "with open('/content/1661-0.txt', 'r', encoding='utf-8') as text_file:\n",
        "    raw_text = text_file.read()\n",
        "\n",
        "# Converting text to lowercase for uniformity\n",
        "raw_text = raw_text.lower()\n",
        "\n",
        "# Initializing the tokenizer\n",
        "word_tokenizer = Tokenizer()\n",
        "word_tokenizer.fit_on_texts([raw_text])\n",
        "total_unique_words = len(word_tokenizer.word_index) + 1\n",
        "\n",
        "# Preparing input sequences for training\n",
        "input_sequences = []\n",
        "for line in raw_text.split('\\n'):\n",
        "    tokenized_line = word_tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(tokenized_line)):\n",
        "        n_gram_sequence = tokenized_line[:i + 1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad the sequences to ensure uniform input size\n",
        "max_length = max(len(seq) for seq in input_sequences)\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_length, padding='pre')\n",
        "\n",
        "# Separating the input data (X) and labels (y)\n",
        "X_data, y_data = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "y_data = keras.utils.to_categorical(y_data, num_classes=total_unique_words)\n",
        "\n",
        "# Building the RNN model\n",
        "rnn_model = keras.Sequential()\n",
        "rnn_model.add(keras.layers.Embedding(total_unique_words, 100, input_length=max_length - 1))\n",
        "rnn_model.add(keras.layers.SimpleRNN(150))\n",
        "rnn_model.add(keras.layers.Dense(total_unique_words, activation='softmax'))\n",
        "\n",
        "# Compiling the model\n",
        "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model on the prepared data\n",
        "rnn_model.fit(X_data, y_data, epochs=10, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_n_words(model, tokenizer, input_text, n=3):\n",
        "    input_text = input_text.lower()\n",
        "    tokenized_input = tokenizer.texts_to_sequences([input_text])[0]\n",
        "    padded_input = pad_sequences([tokenized_input], maxlen=max_length - 1, padding='pre')\n",
        "\n",
        "    predicted_probs = model.predict(padded_input, verbose=0)\n",
        "\n",
        "    # Get the top N predicted words\n",
        "    top_n_indices = np.argsort(predicted_probs[0])[-n:][::-1]  # Get the top N indices\n",
        "    top_n_words = [tokenizer.index_word[index] for index in top_n_indices]\n",
        "\n",
        "    return top_n_words\n",
        "\n",
        "# Example usage for predicting the top N next words\n",
        "input_phrase = 'This American had started from London when he was young, and he wanted to do the'\n",
        "top_n_words = get_top_n_words(rnn_model, word_tokenizer, input_phrase, n=5)\n",
        "print('Top predicted next words:', top_n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIh6FUoUK8Pj",
        "outputId": "1c3e8365-b293-47ca-bb9e-d2eadd036936"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predicted next words: ['same', 'old', 'inspector', 'truth', 'writing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = 'I never hope to see such a'\n",
        "top_n_words = get_top_n_words(rnn_model, word_tokenizer, input_phrase, n=5)\n",
        "print('Top predicted next words:', top_n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVo-lFiULWsD",
        "outputId": "9057aafa-6a95-44c0-fbd6-f3d4761bab00"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predicted next words: ['lady', 'sight', 'work', 'single', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = 'There was nothing in the'\n",
        "top_n_words = get_top_n_words(rnn_model, word_tokenizer, input_phrase, n=5)\n",
        "print('Top predicted next words:', top_n_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1c2aJ8dRL42",
        "outputId": "5198ab57-04b4-4308-f8b8-e1cd7d0bf811"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top predicted next words: ['same', 'disappearance', 'county', 'centre', 'first']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implementation uses a Recurrent Neural Network (RNN) to predict the next word in a given text sequence. It starts by preparing the text data and creating input sequences for training. The model consists of an embedding layer, an RNN layer, and a dense output layer, trained to predict the next word based on context. After training, we can input a phrase to receive the top possible next words.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wvoieqz6VxKK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FbDZhOnXV4O-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}